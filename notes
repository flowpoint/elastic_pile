
def chunk_document(document):
    try:
        if isinstance(document, dict):
            doc = document["text"]
        else:
            doc = document

        splits = doc.split()
        # if the splits are suspiciously small, it wasn't split well, 
        # 16 characters per word is suspicious, 8 char is avg for english
        # use character based chunking instead of word based
        if len(splits) * 16 < len(doc):
            res = sliced(doc, 8)
        else:
            res = splits


        # return -1 as word positions for this pre_tokenizer

        try:
            # important that some of these maps are evaluated or they spam your memory
            chunks = list(chunked(res, 256))

        except Exception as e:
            logger.error(f"chunker encoding failed with:\n{truncate_error(e)}")
            return [""]

    except Exception as e:
        logger.error(f"chunker decoding failed with:\n{truncate_error(e)}")
        return [""]

    s = list(map(lambda chunk: " ".join(chunk), chunks))
    return s
